master:
  prefix: deep_bert_test # bert | roberta | bert-tweet | word2vec | fasttext
  release_mode: patch
  description: test # This is model description
  train_key: train
  dev_key: dev
  test_key: test
  save_path: D:\WorkBench\TA NLP\res_deep # save folder path
  sampling: true # sampling toggle
  labels: Uncorrelated_Contra Sarcasm_Pro_Neutral_Contra_Pro Sarcasm
  label_key: Label
  key_list: Tweet_Comment
loader:
  data_path: D:\WorkBench\TA NLP\dataset_splitted\rey_normal # path to data
  tokenizer_type: roberta # bert | roberta | bert-tweet | word2vec | fasttext
  model_name: cardiffnlp/twitter-xlm-roberta-base # indobenchmark/indobert-base-p2 | cardiffnlp/twitter-xlm-roberta-base | indolem/indobertweet-base-uncased
  tokenizer_config:
    max_length: 512
    padding: max_length
    return_tensors: tf
    truncation: true
    # for word vectors
    # model_type: word2vec # word2vec | fasttext
    # is_pretrained: true
    # model_path: D:\WorkBench\TA NLP\pretrained_models\w2v\word2vec.model #path to model
    # log_oov: false
    # vector_size: 100 
    # max_sequence: 256 # max_length/2
    # model_behavior: concat # concat | combine
classifier:
  type: roberta # bert | roberta | bert-tweet | word2vec | fasttext
  model_name: cardiffnlp/twitter-xlm-roberta-base # indobenchmark/indobert-base-p2 | cardiffnlp/twitter-xlm-roberta-base | indolem/indobertweet-base-uncased
  recurrent_layer: lstm # lstm | bilstm | gru | bigru
  recurrent_unit: 64
  recurrent_dropout: 0.4
trainer:
  gpus: 1 | 2 | 3
  batch_size: 4
  learning_rate: 5.0e-05
  epochs: 2
  freeze_embedding: true
  train_freeze:
    learning_rate: 1.0e-04
    batch_size: 4
    epochs: 2





