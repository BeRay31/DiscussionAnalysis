master:
  prefix: deep_word2vec # bert | xlnet | word2vec | fasttext
  release_mode: patch
  description: test # This is model description
  train_key: train
  dev_key: dev
  test_key: test
  save_path: D:\WorkBench\TA NLP\res_deep # save folder path
  sampling: true # sampling toggle
  labels: Uncorrelated_Contra Sarcasm_Pro_Neutral_Contra_Pro Sarcasm
  label_key: Label
  key_list: Tweet_Comment
loader:
  data_path: D:\WorkBench\TA NLP\dataset_splitted\rey_normal # path to data
  test_data_path: 'D:\WorkBench\TA NLP\dataset_splitted\test.csv'
  tokenizer_type: word2vec # bert | xlnet | word2vec | fasttext
  model_name: indobenchmark/indobert-base-p2 # indobenchmark/indobert-base-p2 | malay-huggingface/xlnet-tiny-bahasa-cased
  tokenizer_config:
    max_length: 512
    padding: max_length
    return_tensors: tf
    truncation: true
    # for word vectors
    model_type: word2vec # word2vec | fasttext
    is_pretrained: true
    model_path: D:\WorkBench\TA NLP\pretrained_models\w2v\word2vec.model #path to model
    log_oov: false
    vector_size: 100 
    max_sequence: 256 # max_length/2
    model_behavior: concat # concat | combine
classifier:
  type: word2vec # bert | xlnet | word2vec | fasttext
  model_name: indobenchmark/indobert-base-p2 | malay-huggingface/xlnet-tiny-bahasa-cased
  recurrent_layer: lstm # lstm | bilstm | gru | bigru
  recurrent_unit: 64
  recurrent_dropout: 0.4
trainer:
  batch_size: 4
  learning_rate: 5.0e-05
  epochs: 20
  gpus: 1 | 2 | 3




