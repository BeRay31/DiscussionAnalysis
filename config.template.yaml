loader:
  features: subtlex-us-num-films-log|subtlex-uk-cbeebies|count-morphemes|subtlex-us-percentage-films|zipf|count-lemma-brown|subtlex-us-num-films-low|subtlex-us-freq-low|subtlex-us-freq-mil|count-length|count-hypernyms|complexity-score|count-min-dist-hypernyms|count-smog-index|count-max-dist-hypernyms|subtlex-us-freq|is-acronym|age-of-acquisition|count-hyponyms|count-lix-score|years-of-appearance|count-synonym|subtlex-uk-cbbc|count-lemma-length|count-coleman-liau-index|count-kincaid-grade-score|volume-count|subtlex-uk-freq|count-stopword|count-freq-brown|count-flesch-score|subtlex-us-freq-log|google-freq
  tokenizer_config:
    max_length: 512
    padding: max_length
    return_tensors: tf
    truncation: true
master:
  enhance_feat: true
  model_name: bert-base-uncased
  type: bert
mode: patch
regressor:
  extractor_unit: 32
  feat_dropout: 0.4
  feat_unit: 8
  layer_type: dense
  text_dropout: 0.4
  text_unit: 64
trainer:
  batch_size: 4
  early_stopping_patience: 5
  epochs: 20
  gpus: 2|3|4|5
  lr: 5.0e-05
  separate_train:
    batch_size: 16
    early_stopping_patience: 10
    epochs: 2
    lr: 0.0001
  train_separately: true
