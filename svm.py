# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SX0ndFEZPcm9DFgHHTNOHV9LZotCLzcP
"""

from google.colab import drive
drive.mount('/content/drive')

"""
## Import Library and Dataset

"""

import nltk
nltk.download('punkt')
!pip install tweet-preprocessor
!pip install --upgrade gensim

import sklearn
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re
from nltk.tokenize import word_tokenize
import gensim
from sklearn.svm import SVC
import time
from sklearn.decomposition import PCA
import pickle
import preprocessor as tp
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from IPython.display import display

tp.set_options(tp.OPT.URL, tp.OPT.MENTION, tp.OPT.URL, tp.OPT.EMOJI, tp.OPT.SMILEY)

dataLocation = "/content/drive/MyDrive/TA/dataset.csv"
dataset = pd.read_csv(dataLocation)

dataset.head()

"""## Data Preparation"""

# Rename header
newHeader = {
    "Tweet": "Tweet",
    "Quote Tweet | Reply": "Comment",
    "Tweet Domain": "Domain",
    "Reyvan": "Label"
}
renamedDs =dataset.rename(columns=newHeader)

"""## EDA"""

def visualizeLabelsDistribution(df):
  labels = df["Label"].unique().tolist()
  labels.sort()
  values = []
  for label in labels:
    values.append(sum(df["Label"] == label))

  colors = sns.color_palette('pastel')
  plt.pie(values, labels = labels, colors = colors, autopct='%1.1f%%')
  plt.show()

visualizeLabelsDistribution(renamedDs)

## Check null value
renamedDs.isnull().sum()

"""## Preprocessing"""

def preprocess_tweet(tweet):
  tempTweet = tp.tokenize(tweet)
  return tempTweet

for idx, row in renamedDs.iterrows():
  renamedDs.loc[idx, "Tweet"] = preprocess_tweet(renamedDs.loc[idx, "Tweet"])
  renamedDs.loc[idx, "Comment"] = preprocess_tweet(renamedDs.loc[idx, "Comment"])

renamedDs.head(10)

"""## Split test and train

"""

# seq length to normalize the vector
def textToVector(model, text, seq_length=101):
    tokens = word_tokenize(text)
    len_v = len(tokens)-1 if len(tokens) < seq_length else seq_length-1
    vec = []
    for tok in tokens[:len_v]:
      vec.append(model.wv[tok])
    last_pieces = seq_length - len(vec)
    for i in range(last_pieces):
      vec.append(np.zeros(100,))
    return np.asarray(vec).flatten()

def get_vector_array(df, w2vModel, max_corpus_length, tweetKey = 'tweet', commentKey = 'comment', concat = True):
  vector = []
  for idx, row in df.iterrows():
    tweet_vec = textToVector(w2vModel, row[tweetKey], max_corpus_length)
    comment_vec = textToVector(w2vModel, row[commentKey], max_corpus_length)
    final_vec = tweet_vec.tolist() + comment_vec.tolist() if (concat) else tweet_vec + comment_vec
    vector.append(final_vec)

  vector = np.asarray(vector)
  return vector

# W2V
def getw2VModel(df, name="w2vModel.word2vec"):
  uniqueTweets = df['Tweet'].unique().tolist()
  uniqueComments = df['Comment'].unique().tolist()
  corpus = uniqueTweets + uniqueComments
  corp_tok = [word_tokenize(sent) for sent in corpus]
  max_corpus_length = max([len(i) for i in corp_tok])
  w2vModel = gensim.models.Word2Vec(corp_tok, vector_size=100, window=5, min_count=1, workers=4, seed=13518136)
  w2vModel.save(name)
  return w2vModel, max_corpus_length

def getFastTextModel(df, name="fasttext.fasttext"):
  uniqueTweets = df['Tweet'].unique().tolist()
  uniqueComments = df['Comment'].unique().tolist()
  corpus = uniqueTweets + uniqueComments
  corp_tok = [word_tokenize(sent) for sent in corpus]
  max_corpus_length = max([len(i) for i in corp_tok])
  fasttextModel = gensim.models.FastText(corp_tok, vector_size=100, window=5, min_count=1, workers=4, seed=13518136)
  fasttextModel.save(name)
  return fasttextModel, max_corpus_length

# FastText
def getModelAndAccuracy(x_train, y_train, x_test, y_test, w2vModel, max_corpus_length, isConcatenated = True, pca_components=50):
  start = time.time()
  x_train_vector = get_vector_array(pd.DataFrame(x_train, columns=['tweet', 'comment']), w2vModel, max_corpus_length, 'tweet', 'comment', isConcatenated)
  milestone_1 = time.time()

  x_test_vector = get_vector_array(pd.DataFrame(x_test, columns=['tweet', 'comment']), w2vModel, max_corpus_length, 'tweet', 'comment', isConcatenated)
  milestone_2 = time.time()
  
  pca_model = PCA(n_components=pca_components, random_state=13518136)
  pca_model.fit(x_train_vector)
  milestone_3 = time.time()
  
  x_train_pca = pca_model.transform(x_train_vector)
  milestone_4 = time.time()
  
  x_test_pca = pca_model.transform(x_test_vector)
  milestone_5 = time.time()

  svm_classifier = SVC(random_state=13618136)
  svm_classifier.fit(x_train_pca, y_train)
  finish_train = time.time()

  score = svm_classifier.score(x_test_pca, y_test)
  process = round(finish_train-milestone_5,2)
  
  print("Is tweet and comment word vector concatenated = {}".format(isConcatenated))
  print("Get word vector for train data was finish in {}".format(round(milestone_1 - start, 2)))
  print("Get word vector for test data was finish in {}".format(round(milestone_2 - milestone_1, 2)))
  print("Sum of variance ratios: {}\ntime spend for PCA train: {}".format(sum(pca_model.explained_variance_ratio_), round(milestone_3 - milestone_2, 2)))
  print("Transform train data with PCA was finish in {}".format(round(milestone_4 - milestone_3, 2)))
  print("Transform test data with PCA was finish in {}".format(round(milestone_5 - milestone_4, 2)))
  print("Support Vector Machine Classifier has fitted, this process took {} seconds with score {}".format(process, score))
  return svm_classifier, pca_model

def save_model(model, name):
  pickle_out = open(name, "wb")
  pickle.dump(model, pickle_out)
  pickle_out.close()

def predictAndSave(model, pca_model, w2vModel, max_corpus_length, x_test, y_test, isConcatenated, name):
  start = time.time()
  x_test_vector = get_vector_array(pd.DataFrame(x_test, columns=['tweet', 'comment']), w2vModel, max_corpus_length, 'tweet', 'comment', isConcatenated)
  x_test_pca = pca_model.transform(x_test_vector)
  pred = model.predict(x_test_pca)
  end = time.time()
  pred = pd.DataFrame(pred)
  x_test = pd.DataFrame(x_test)
  y_test = pd.DataFrame(y_test)
  process = round(end - start, 2)
  print("Prediction process of {} took {} seconds".format(name, process))
  df = pd.concat([x_test, y_test, pred], axis = 1)
  df.columns = ["Tweet", "Comment", "Label", "Prediction"]
  df.to_csv(name)

y = renamedDs["Label"]
x = renamedDs.drop(["Label", "No", "Domain"], axis=1)

# ROS
ros = RandomOverSampler(random_state=13518136)
X_resampled_ros, y_resampled_ros = ros.fit_resample(x, y)

# RUS
rus = RandomUnderSampler(random_state=13518136)
X_resampled_rus, y_resampled_rus = rus.fit_resample(x, y)

rosDs = pd.concat([X_resampled_ros, y_resampled_ros], axis = 1)
visualizeLabelsDistribution(rosDs)
len(rosDs)

rusDs = pd.concat([X_resampled_rus, y_resampled_rus], axis = 1)
visualizeLabelsDistribution(rusDs)
len(rusDs)

# W2V model
w2vModel, max_corpus_length = getw2VModel(renamedDs, "w2vModel.word2vec")
print("max_corpus_length = ", max_corpus_length)
# ROS
w2vModel_ros, max_corpus_length_ros = getw2VModel(rosDs, "w2vModel_ros.word2vec")
print("max_corpus_length ROS = ", max_corpus_length_ros)

# RUS
w2vModel_rus, max_corpus_length_rus = getw2VModel(rusDs, "w2vModel_rus.word2vec")
print("max_corpus_length RUS = ", max_corpus_length_rus)

# FastText Model

fastTextModel, max_corpus_length = getFastTextModel(renamedDs, "fastTextModel.fastText")
print("max_corpus_length = ", max_corpus_length)
# ROS
fastTextModel_ros, max_corpus_length_ros = getFastTextModel(rosDs, "fastTextModel_ros.fastText")
print("max_corpus_length ROS = ", max_corpus_length_ros)

# RUS
fastTextModel_rus, max_corpus_length_rus = getFastTextModel(rusDs, "fastTextModel_rus.fastText")
print("max_corpus_length RUS = ", max_corpus_length_rus)

y = renamedDs["Label"]
x = renamedDs.drop(["Label", "No", "Domain"], axis=1)

# Normal
x_train, x_test, y_train, y_test = train_test_split(x.values, y, random_state=13518136, test_size=0.33)
# ROS
x_train_ros, x_test_ros, y_train_ros, y_test_ros = train_test_split(X_resampled_ros.values, y_resampled_ros, random_state=13518136, test_size=0.33)
# RUS
x_train_rus, x_test_rus, y_train_rus, y_test_rus = train_test_split(X_resampled_rus.values, y_resampled_rus, random_state=13518136, test_size=0.33)

"""## Word2Vec Embedding"""

print()
print("========\tSVM with concated word vector\t========")
print()
svm_concat_normal, pca_concat_normal = getModelAndAccuracy(x_train, y_train, x_test, y_test, w2vModel, max_corpus_length)
print()
print("========\tSVM with combined word vector\t========")
print()
svm_combined_normal, pca_combined_normal = getModelAndAccuracy(x_train, y_train, x_test, y_test, w2vModel, max_corpus_length, False)

print()
print("========\tSVM with concated word vector ROS\t========")
print()
svm_concat_ROS, pca_concat_ROS = getModelAndAccuracy(x_train_ros, y_train_ros, x_test_ros, y_test_ros, w2vModel_ros, max_corpus_length_ros)
print()
print("========\tSVM with combined word vector ROS\t========")
print()
svm_combined_ROS, pca_combined_ROS = getModelAndAccuracy(x_train_ros, y_train_ros, x_test_ros, y_test_ros, w2vModel_ros, max_corpus_length_ros, False)

print()
print("========\tSVM with concated word vector RUS\t========")
print()
svm_concat_RUS, pca_concat_RUS = getModelAndAccuracy(x_train_rus, y_train_rus, x_test_rus, y_test_rus, w2vModel_rus, max_corpus_length_rus)
print()
print("========\tSVM with combined word vector RUS\t========")
print()
svm_combined_RUS, pca_combined_RUS = getModelAndAccuracy(x_train_rus, y_train_rus, x_test_rus, y_test_rus, w2vModel_rus, max_corpus_length_rus, False)

# save_model(svm_concat_normal, "svm_concat_normal.pickle")
# save_model(svm_combined_normal, "svm_combined_normal.pickle")

# save_model(svm_concat_ROS, "svm_concat_ROS.pickle")
# save_model(svm_combined_ROS, "svm_combined_ROS.pickle")

# save_model(svm_concat_RUS, "svm_concat_RUS.pickle")
# save_model(svm_combined_RUS, "svm_combined_RUS.pickle")

# save_model(pca_concat_normal, "pca_concat_normal.pickle")
# save_model(pca_combined_normal, "pca_combined_normal.pickle")

# save_model(pca_concat_ROS, "pca_concat_ROS.pickle")
# save_model(pca_combined_ROS, "pca_combined_ROS.pickle")

# save_model(pca_concat_RUS, "pca_concat_RUS.pickle")
# save_model(pca_combined_RUS, "pca_combined_RUS.pickle")

# def predictAndSave(model, pca_model, w2vModel, max_corpus_length, x_test, y_test, isConcatenated, name):
predictAndSave(svm_concat_normal, pca_concat_normal, w2vModel, max_corpus_length, x_test, y_test, True, "svm_concat_normal.csv")
predictAndSave(svm_combined_normal, pca_combined_normal, w2vModel, max_corpus_length, x_test, y_test, False, "svm_combined_normal.csv")

predictAndSave(svm_concat_ROS, pca_concat_ROS, w2vModel_ros, max_corpus_length_ros, x_test_ros, y_test_ros, True, "svm_concat_ROS.csv")
predictAndSave(svm_combined_ROS, pca_combined_ROS, w2vModel_ros, max_corpus_length_ros, x_test_ros, y_test_ros, False, "svm_combined_ROS.csv")

predictAndSave(svm_concat_RUS, pca_concat_RUS, w2vModel_rus, max_corpus_length_rus, x_test_rus, y_test_rus, True, "svm_concat_RUS.csv")
predictAndSave(svm_combined_RUS, pca_combined_RUS, w2vModel_rus, max_corpus_length_rus, x_test_rus, y_test_rus, False, "svm_combined_RUS.csv")

# print(len(x_test))
# print(len(x_test_ros))
# print(len(x_test_rus))

"""## FastText Embedding"""

print()
print("========\tSVM with concated word vector with fasttext\t========")
print()
svm_concat_normal_fasttext, pca_concat_normal_fasttext = getModelAndAccuracy(x_train, y_train, x_test, y_test, fastTextModel, max_corpus_length)
print()
print("========\tSVM with combined word vector with fasttext\t========")
print()
svm_combined_normal_fasttext, pca_combined_normal_fasttext = getModelAndAccuracy(x_train, y_train, x_test, y_test, fastTextModel, max_corpus_length, False)

print()
print("========\tSVM with concated word vector with fasttext ROS\t========")
print()
svm_concat_ROS_fasttext, pca_concat_ROS_fasttext = getModelAndAccuracy(x_train_ros, y_train_ros, x_test_ros, y_test_ros, fastTextModel_ros, max_corpus_length_ros)
print()
print("========\tSVM with combined word vector with fasttext ROS\t========")
print()
svm_combined_ROS_fasttext, pca_combined_ROS_fasttext = getModelAndAccuracy(x_train_ros, y_train_ros, x_test_ros, y_test_ros, fastTextModel_ros, max_corpus_length_ros, False)

print()
print("========\tSVM with concated word vector with fasttext RUS\t========")
print()
svm_concat_RUS_fasttext, pca_concat_RUS_fasttext = getModelAndAccuracy(x_train_rus, y_train_rus, x_test_rus, y_test_rus, fastTextModel_rus, max_corpus_length_rus)
print()
print("========\tSVM with combined word vector with fasttext RUS\t========")
print()
svm_combined_RUS_fasttext, pca_combined_RUS_fasttext = getModelAndAccuracy(x_train_rus, y_train_rus, x_test_rus, y_test_rus, fastTextModel_rus, max_corpus_length_rus, False)

save_model(svm_concat_normal_fasttext, "svm_concat_normal_fasttext.pickle")
save_model(svm_combined_normal_fasttext, "svm_combined_normal_fasttext.pickle")

save_model(svm_concat_ROS_fasttext, "svm_concat_ROS_fasttext.pickle")
save_model(svm_combined_ROS_fasttext, "svm_combined_ROS_fasttext.pickle")

save_model(svm_concat_RUS_fasttext, "svm_concat_RUS_fasttext.pickle")
save_model(svm_combined_RUS_fasttext, "svm_combined_RUS_fasttext.pickle")

save_model(pca_concat_normal_fasttext, "pca_concat_normal_fasttext.pickle")
save_model(pca_combined_normal_fasttext, "pca_combined_normal_fasttext.pickle")

save_model(pca_concat_ROS_fasttext, "pca_concat_ROS_fasttext.pickle")
save_model(pca_combined_ROS_fasttext, "pca_combined_ROS_fasttext.pickle")

save_model(pca_concat_RUS_fasttext, "pca_concat_RUS_fasttext.pickle")
save_model(pca_combined_RUS_fasttext, "pca_combined_RUS_fasttext.pickle")

# # def predictAndSave(model, pca_model, w2vModel, max_corpus_length, x_test, y_test, isConcatenated, name):
predictAndSave(svm_concat_normal_fasttext, pca_concat_normal_fasttext, fastTextModel, max_corpus_length, x_test, y_test, True, "svm_concat_normal_fasttext.csv")
predictAndSave(svm_combined_normal_fasttext, pca_combined_normal_fasttext, fastTextModel, max_corpus_length, x_test, y_test, False, "svm_combined_normal_fasttext.csv")

predictAndSave(svm_concat_ROS_fasttext, pca_concat_ROS_fasttext, fastTextModel_ros, max_corpus_length_ros, x_test_ros, y_test_ros, True, "svm_concat_ROS_fasttext.csv")
predictAndSave(svm_combined_ROS_fasttext, pca_combined_ROS_fasttext, fastTextModel_ros, max_corpus_length_ros, x_test_ros, y_test_ros, False, "svm_combined_ROS_fasttext.csv")

predictAndSave(svm_concat_RUS_fasttext, pca_concat_RUS_fasttext, fastTextModel_rus, max_corpus_length_rus, x_test_rus, y_test_rus, True, "svm_concat_RUS_fasttext.csv")
predictAndSave(svm_combined_RUS_fasttext, pca_combined_RUS_fasttext, fastTextModel_rus, max_corpus_length_rus, x_test_rus, y_test_rus, False, "svm_combined_RUS_fasttext.csv")

predictAndSave(svm_combined_ROS_fasttext, pca_combined_ROS_fasttext, fastTextModel_ros, max_corpus_length_ros, x_test_ros, y_test_ros, False, "/content/drive/MyDrive/TA/svm_combined_ROS_fasttext.csv")